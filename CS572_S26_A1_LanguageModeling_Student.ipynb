{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Zg1qMBn7YQk"
      },
      "source": [
        "## Assignment 1: Language Modeling\n",
        "\n",
        "*This assignment is adapted from one created by David Gaddy, Daniel Fried, Nikita Kitaev, Mitchell Stern, Rodolfo Corona, John DeNero, and Dan Klein.*\n",
        "\n",
        "In this assignment, you will implement several different types of language models for text.  We'll start with n-gram models, then move on to neural n-gram and transformer language models.\n",
        "\n",
        "**Warning**: Do not start this assignment the day before it is due!  Some parts require 20 minutes or more to run, so debugging and tuning can take a significant amount of time.\n",
        "\n",
        "Our dataset for this assignment will be the WikiText2 language modeling dataset.  This dataset comes with some of the basic preprocessing done for us, such as tokenization and rare word filtering (using the `<unk>` token).\n",
        "Therefore, we can assume that all word types in the test set also appear at least once in the training set.\n",
        "We'll also use the Huggingface `datasets` and `tokenizers` libraries to help with some of the data preprocessing, such as converting tokens into id numbers.\n",
        "\n",
        "**Note on GPU usage**: You will need to use a GPU for the neural n-gram and transformer models but **not** for the n-gram models. Colab places some restrictions on GPU usage due to which you might get locked out after continuously using one (~8 hours). To avoid this, you should only use the GPU when needed, i.e., on training and inference for the last two parts of this assignment. You can enable / disable GPU usage by changing the Runtime type under the Runtime menu.\n",
        "If you do get locked out of using a GPU, a potential workaround is to sign in using a different account.\n",
        "\n",
        "When training a model on the GPU it is also a good idea to save your model periodically in case you get locked out. You can use `torch.save(network.state_dict(), path)` and `network.load_state_dict()` for this; see [here](https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_models_for_inference.html).\n",
        "\n",
        "**Grading rubric**\n",
        "- 70% Results\n",
        "    - 15% bigram_predictions.npy (correctness)\n",
        "    - 15% trigram_backoff_predictions.npy (correctness)\n",
        "    - 15% neural_trigram_predictions.npy (meets target)\n",
        "    - 15% transformer_predictions.npy (meets target)\n",
        "    - 10% transformer_predictions.npy (improvement over target)\n",
        "- 30% Report\n",
        "    - 25% Experimentation (Clarity, Correctness, and Analysis)\n",
        "    - 5% Discussion of errors and Learning experience"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQZGzVihrDUw"
      },
      "outputs": [],
      "source": [
        "# Install some required packages.\n",
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install torch==2.3.0 torchtext==0.18.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2oXAsamCqtH3"
      },
      "outputs": [],
      "source": [
        "# This block handles some basic setup and data loading.\n",
        "# You shouldn't need to edit this, but if you want to\n",
        "# import other standard python packages, that is fine.\n",
        "\n",
        "from collections import defaultdict, Counter\n",
        "import numpy as np\n",
        "import math\n",
        "import tqdm\n",
        "import random\n",
        "import pdb\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torchtext\n",
        "import torch.optim as optim\n",
        "\n",
        "# We'll use HuggingFace's datasets and tokenizers libraries, which are a bit\n",
        "# heavy-duty for what we're doing, but it's worth getting to know them.\n",
        "\n",
        "import transformers\n",
        "\n",
        "from datasets import load_dataset\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import WordLevel\n",
        "from tokenizers.trainers import WordLevelTrainer\n",
        "from tokenizers.pre_tokenizers import WhitespaceSplit\n",
        "\n",
        "dataset = load_dataset(\"Salesforce/wikitext\", \"wikitext-2-v1\")\n",
        "tokenizer = Tokenizer(WordLevel(unk_token='<unk>'))\n",
        "tokenizer.pre_tokenizer = WhitespaceSplit() # should be equivalent to split()\n",
        "\n",
        "# \"Training\" a tokenizer below just feeds it all the tokens so it can map from\n",
        "# word type to id.\n",
        "\n",
        "trainer = WordLevelTrainer( # should only be 33,278 distinct types in Wikitext-2\n",
        "    vocab_size=33300, special_tokens=[\"<unk>\", \"<eos>\"])\n",
        "generator_bsz = 512\n",
        "all_splits_generator = (dataset[split][i:i+generator_bsz][\"text\"]\n",
        "                        for split in [\"train\", \"validation\", \"test\"]\n",
        "                          for i in range (0, len(dataset[split]), generator_bsz))\n",
        "tokenizer.train_from_iterator(all_splits_generator, trainer)\n",
        "\n",
        "# If desired, we could make a transformers tokenizer object now with:\n",
        "# fast_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer)\n",
        "\n",
        "orig_vocab = tokenizer.get_vocab() # The tokenizer reserves a <pad> id, which we'll ignore.\n",
        "word_types = sorted(list(orig_vocab.keys()), key=lambda w: orig_vocab[w]) # no <pad>\n",
        "vocab = {w: i for i, w in enumerate(word_types)} # no <pad>\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# Make a single stream of tokens, with an <eos> after each newline.\n",
        "\n",
        "train_text = []\n",
        "for example in dataset[\"train\"][\"text\"]:\n",
        "  train_text.extend(tokenizer.encode(example).tokens + [\"<eos>\"])\n",
        "\n",
        "validation_text = []\n",
        "for example in dataset[\"validation\"][\"text\"]:\n",
        "  validation_text.extend(tokenizer.encode(example).tokens + [\"<eos>\"])\n",
        "\n",
        "print(validation_text[:30])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g10PLGiZn0XY"
      },
      "source": [
        "We've implemented a unigram model here as a demonstration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7ZHMVZzoPEH"
      },
      "outputs": [],
      "source": [
        "class UnigramModel:\n",
        "    def __init__(self, train_text):\n",
        "        self.counts = Counter(train_text)\n",
        "        self.total_count = len(train_text)\n",
        "\n",
        "    def probability(self, word):\n",
        "        return self.counts[word] / self.total_count\n",
        "\n",
        "    def next_word_probabilities(self, text_prefix):\n",
        "        \"\"\"Return a list of probabilities for each word in the vocabulary.\"\"\"\n",
        "        return [self.probability(word) for word in word_types]\n",
        "\n",
        "    def perplexity(self, full_text):\n",
        "        \"\"\"Return the perplexity of the model on a text as a float.\n",
        "\n",
        "        full_text -- a list of string tokens\n",
        "        \"\"\"\n",
        "        log_probabilities = []\n",
        "        for word in full_text:\n",
        "            # Note that the base of the log doesn't matter\n",
        "            # as long as the log and exp use the same base.\n",
        "            log_probabilities.append(math.log(self.probability(word), 2))\n",
        "        return 2 ** -np.mean(log_probabilities)\n",
        "\n",
        "unigram_demonstration_model = UnigramModel(train_text)\n",
        "print('unigram validation perplexity:',\n",
        "      unigram_demonstration_model.perplexity(validation_text))\n",
        "\n",
        "def check_validity(model):\n",
        "    \"\"\"Performs several sanity checks on your model:\n",
        "    1) That next_word_probabilities returns a valid distribution\n",
        "    2) That perplexity matches a perplexity calculated from next_word_probabilities\n",
        "\n",
        "    Although it is possible to calculate perplexity from next_word_probabilities,\n",
        "    it is still good to have a separate more efficient method that only computes\n",
        "    the probabilities of observed words.\n",
        "    \"\"\"\n",
        "\n",
        "    log_probabilities = []\n",
        "    for i in range(10):\n",
        "        prefix = validation_text[:i]\n",
        "        probs = model.next_word_probabilities(prefix)\n",
        "        assert min(probs) >= 0, \"Negative value in next_word_probabilities\"\n",
        "        assert max(probs) <= 1 + 1e-8, \"Value larger than 1 in next_word_probabilities\"\n",
        "        assert abs(sum(probs)-1) < 1e-4, \"next_word_probabilities do not sum to 1\"\n",
        "\n",
        "        word_id = vocab[validation_text[i]]\n",
        "        selected_prob = probs[word_id]\n",
        "        log_probabilities.append(math.log(selected_prob))\n",
        "\n",
        "    perplexity = math.exp(-np.mean(log_probabilities))\n",
        "    your_perplexity = model.perplexity(validation_text[:10])\n",
        "    assert abs(perplexity-your_perplexity) < 0.1, \"your perplexity does not \" + \\\n",
        "    \"match the one we calculated from `next_word_probabilities`,\\n\" + \\\n",
        "    \"at least one of `perplexity` or `next_word_probabilities` is incorrect.\\n\" + \\\n",
        "    f\"we calcuated {perplexity} from `next_word_probabilities`,\\n\" + \\\n",
        "    f\"but your perplexity function returned {your_perplexity} (on a small sample).\"\n",
        "\n",
        "\n",
        "check_validity(unigram_demonstration_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4esz5XrEpNo"
      },
      "source": [
        "To generate from a language model, we can sample one word at a time conditioning on the words we have generated so far."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bfNj5nl4E7Zn"
      },
      "outputs": [],
      "source": [
        "def generate_text(model, n=20, prefix=('<eos>', '<eos>')):\n",
        "    prefix = list(prefix)\n",
        "    for _ in range(n):\n",
        "        probs = model.next_word_probabilities(prefix)\n",
        "        word = random.choices(word_types, probs)[0]\n",
        "        prefix.append(word)\n",
        "    return ' '.join(prefix)\n",
        "\n",
        "print(generate_text(unigram_demonstration_model))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wq-WtaM6F6kN"
      },
      "source": [
        "In fact there are many strategies to get better-sounding samples, such as only sampling from the top-k words or sharpening the distribution with a temperature.  You can read more about sampling from a language model in this paper: https://arxiv.org/pdf/1904.09751.pdf."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuopg4rYjf2O"
      },
      "source": [
        "You will need to submit some outputs from the models you implement for us to grade.  The following will download the required output files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7kL3zQDVw7g9"
      },
      "outputs": [],
      "source": [
        "!gdown --id 1QuBohoToduXKKJRCQAbkPl7Is73uGbdA\n",
        "!gdown --id 1ZlXcG7FlG7YxjUMxqOD-KjtoKW2iTyuE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZB6MKPbm4z9s"
      },
      "outputs": [],
      "source": [
        "def save_truncated_distribution(model, filename, short=True):\n",
        "    \"\"\"Generate a file of truncated distributions.\n",
        "\n",
        "    Probability distributions over the full vocabulary are large,\n",
        "    so we will truncate the distribution to a smaller vocabulary.\n",
        "\n",
        "    Please do not edit this function\n",
        "    \"\"\"\n",
        "    vocab_name = 'nu_eval_output_vocab'\n",
        "    prefixes_name = 'nu_eval_prefixes'\n",
        "\n",
        "    if short:\n",
        "      vocab_name += '_short'\n",
        "      prefixes_name += '_short'\n",
        "\n",
        "    with open('{}.txt'.format(vocab_name), 'r') as eval_vocab_file:\n",
        "        eval_vocab = [w.strip() for w in eval_vocab_file]\n",
        "    eval_vocab_ids = [vocab[s] for s in eval_vocab]\n",
        "\n",
        "    all_selected_probabilities = []\n",
        "    with open('{}.txt'.format(prefixes_name), 'r') as eval_prefixes_file:\n",
        "        lines = eval_prefixes_file.readlines()\n",
        "        for line in tqdm.notebook.tqdm(lines, leave=False):\n",
        "            prefix = line.strip().split(' ')\n",
        "            probs = model.next_word_probabilities(prefix)\n",
        "            selected_probs = np.array([probs[i] for i in eval_vocab_ids], dtype=np.float32)\n",
        "            all_selected_probabilities.append(selected_probs)\n",
        "\n",
        "    all_selected_probabilities = np.stack(all_selected_probabilities)\n",
        "    np.save(filename, all_selected_probabilities)\n",
        "    print('saved', filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_nzVrTWcH67Q"
      },
      "outputs": [],
      "source": [
        "save_truncated_distribution(unigram_demonstration_model,\n",
        "                            'unigram_demonstration_predictions.npy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpoBn9m8jw0p"
      },
      "source": [
        "**Before you proceed**: At this point you should check whether you are able to upload the submission files to Gradescope. For this we will generate *dummy* prediction files by copying the unigram predictions above. Download the `unigrm_demonstration_predictions.npy` (you can do this by clicking the folder icon on left menu) and then copy this file and rename it to generate the required submision files:\n",
        "* bigram_predictions.npy\n",
        "* trigram_backoff_predictions.npy\n",
        "* neural_trigram_predictions.npy\n",
        "* transformer_predictions.npy\n",
        "\n",
        "Also save a copy of this notebook as `assignment_1.ipynb` and create a `report.pdf` (this can be empty for now). Upload these files to Gradescope and confirm that the autograder runs and produces an output score of 0."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEfUwbbS9vy0"
      },
      "source": [
        "### N-gram Model\n",
        "\n",
        "Now it's time to implement an n-gram language model.\n",
        "\n",
        "Because not every n-gram will have been observed in training, use Lidstone smoothing to make sure no output word has probability 0.\n",
        "\n",
        "$$P(w_2|w_1)=\\frac{\\#(w_1,w_2)+\\alpha}{\\#(w_1)+V\\alpha}$$\n",
        "\n",
        "where $V$ is the vocab size and $\\#()$ is the count for the given bigram.  An alpha value around `3e-3`  should work.  Later, we'll replace this smoothing with model backoff.\n",
        "\n",
        "One edge case you will need to handle is at the beginning of the text where you don't have `n-1` prior words.  You can handle this however you like as long as you produce a valid probability distribution, but just using a uniform distribution over the vocabulary is reasonable for the purposes of this assignment.\n",
        "\n",
        "A properly implemented bi-gram model should get a perplexity below 505 on the validation set.\n",
        "\n",
        "**Note**: Do not change the signature of the `next_word_probabilities` and `perplexity` functions.  We will use these as a common interface for all of the different model types.  Make sure these two functions call `n_gram_probability`, because later we are going to override `n_gram_probability` in a subclass.\n",
        "Also, we suggest pre-computing and caching the counts $C$ when you initialize `NGramModel` for efficiency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGnGpnPIXpTW"
      },
      "outputs": [],
      "source": [
        "class NGramModel:\n",
        "    def __init__(self, train_text, n=2, alpha=3e-3):\n",
        "        # get counts and perform any other setup\n",
        "        self.n = n\n",
        "        self.smoothing = alpha\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "\n",
        "    def _to_string(self, tokens):\n",
        "      return \"_\".join(tokens)\n",
        "\n",
        "    def n_gram_probability(self, n_gram):\n",
        "        \"\"\"Return the probability of the last word in an n-gram.\n",
        "\n",
        "        n_gram -- a list of string tokens\n",
        "        returns the conditional probability of the last token given the rest.\n",
        "        \"\"\"\n",
        "        assert len(n_gram) == self.n\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "\n",
        "    def next_word_probabilities(self, text_prefix):\n",
        "        \"\"\"Return a list of probabilities for each word in the vocabulary.\"\"\"\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "        # use your function n_gram_probability\n",
        "        # Recall word_types contains a list of words to return probabilities for\n",
        "\n",
        "    def perplexity(self, full_text):\n",
        "        \"\"\" full_text is a list of string tokens\n",
        "        return perplexity as a float \"\"\"\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "        # use your function n_gram_probability\n",
        "        # This method should differ a bit from the example unigram model because\n",
        "        # the first n-1 words of full_text must be handled as a special case.\n",
        "\n",
        "unigram_model = NGramModel(train_text, 1)\n",
        "check_validity(unigram_model)\n",
        "print('unigram validation perplexity:', unigram_model.perplexity(validation_text)) # this should be the almost the same as our unigram model perplexity above\n",
        "\n",
        "bigram_model = NGramModel(train_text, n=2)\n",
        "check_validity(bigram_model)\n",
        "print('bigram validation perplexity:', bigram_model.perplexity(validation_text))\n",
        "\n",
        "trigram_model = NGramModel(train_text, n=3)\n",
        "check_validity(trigram_model)\n",
        "print('trigram validation perplexity:', trigram_model.perplexity(validation_text)) # this won't do very well...\n",
        "\n",
        "save_truncated_distribution(bigram_model, 'bigram_predictions.npy') # this might take a few minutes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzRRLnk73-r9"
      },
      "source": [
        "\n",
        "Please download `bigram_predictions.npy` once you finish this section so that you can submit it.\n",
        "\n",
        "In the block below, please report your bigram validation perplexity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEcUK27xVTcK"
      },
      "source": [
        "<!-- Do not remove this comment, it is used by the autograder: RqYJKsoTS6 -->\n",
        "\n",
        "Bigram validation perplexity: ***fill in here***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qs6zgYw9VTx1"
      },
      "source": [
        "We can also generate samples from the model to get an idea of how it is doing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m2V-qHxB4yhS"
      },
      "outputs": [],
      "source": [
        "print(generate_text(bigram_model))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsR8_Ch7AXAZ"
      },
      "source": [
        "We now free up some RAM, **it is important to run the cell below, otherwise you will likely run out of RAM in the Colab runtime.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EjKt1ncf_ypz"
      },
      "outputs": [],
      "source": [
        "# Free up some RAM.\n",
        "del bigram_model\n",
        "del trigram_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWXNlsEKb3Mz"
      },
      "source": [
        "This basic model works okay for bigrams, but a better strategy (especially for higher-order models) is to use backoff.  Implement backoff with absolute discounting.\n",
        "$$P\\left(w_i|h_i\\right)=\\frac{max\\left\\{\\#(h_i, w_i)-d,0\\right\\}}{\\#(h_i)} + \\lambda(h_i) P(w_i|w_{i-n+2},\\ldots, w_{i-1})$$\n",
        "\n",
        "$$\\lambda\\left(h_i\\right)=\\frac{d N_{1+}(h_i)}{{\\#(h_i)}}$$\n",
        "where $h_i=(w_{i-n+1}, \\ldots, w_{i-1})$ is the prefix before token $i$, $N_{1+}$ is the number of words that appear after the previous $n-1$ words (the number of times the max will select something other than 0 in the first equation).  If $\\#(h_i)=0$, use the lower order model probability directly (the above equations would have a division by 0).\n",
        "\n",
        "We found a discount $d$ of 0.9 to work well based on validation performance.  A trigram model with this discount value should get a validation perplexity below 272."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BV4e4_mEc7VY"
      },
      "outputs": [],
      "source": [
        "class DiscountBackoffModel(NGramModel):\n",
        "    def __init__(self, train_text, lower_order_model, n=2, delta=0.9):\n",
        "        super().__init__(train_text, n=n)\n",
        "        self.lower_order_model = lower_order_model\n",
        "        self.discount = delta\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "\n",
        "    def n_gram_probability(self, n_gram):\n",
        "        assert len(n_gram) == self.n\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "        # back off to the lower_order model with n'=n-1 using its n_gram_probability function\n",
        "\n",
        "bigram_backoff_model = DiscountBackoffModel(train_text, unigram_model, 2)\n",
        "trigram_backoff_model = DiscountBackoffModel(train_text, bigram_backoff_model, 3)\n",
        "check_validity(trigram_backoff_model)\n",
        "print('trigram backoff validation perplexity:', trigram_backoff_model.perplexity(validation_text))\n",
        "save_truncated_distribution(trigram_backoff_model, 'trigram_backoff_predictions.npy') # this might take a few minutes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVrWYSMsBRSV"
      },
      "source": [
        "Free up RAM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_WJe_trXBTjN"
      },
      "outputs": [],
      "source": [
        "# Release models we don't need any more.\n",
        "del unigram_model\n",
        "del bigram_backoff_model\n",
        "del trigram_backoff_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5Y0S6XbB1iZ"
      },
      "source": [
        "## Neural N-gram Model\n",
        "\n",
        "In this section, you will implement the training and evaluation logic for a neural n-gram model.\n",
        "\n",
        "**We have provided the model architecture for you in the `NeuralNGramNetwork` class.** It uses a simple feedforward neural network that takes the previous n-1 words and outputs a distribution over the next word.\n",
        "\n",
        "Your task is to complete the `NeuralNGramModel` class. Specifically, you need to implement:\n",
        "1.  **`train`**: The training loop (using the provided `NeuralNgramDataset` and `NeuralNGramNetwork`).\n",
        "2.  **`next_word_probabilities`**: Logic to generate probabilities for a given context.\n",
        "3.  **`perplexity`**: Evaluation logic to compute perplexity on a dataset.\n",
        "\n",
        "The provided `NeuralNGramNetwork` follows this architecture (for your reference):\n",
        "* Embed the words with dimension 128, then flatten into a single embedding for $n-1$ words.\n",
        "* Run 2 hidden layers with 1024 hidden units, with ReLU activation and 0.1 dropout.\n",
        "* Project down to size 128 before the final layer.\n",
        "* Use weight tying for the embedding and final linear layer.\n",
        "\n",
        "**Requirements for your implementation:**\n",
        "* Train for 10 epochs with the Adam optimizer (should take around 15-20 minutes).\n",
        "* Do early stopping based on validation set perplexity.\n",
        "* A correct implementation should reach a validation perplexity below **226**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jokaz820Fk1h"
      },
      "outputs": [],
      "source": [
        "def ids(tokens):\n",
        "    return [vocab[t] for t in tokens]\n",
        "\n",
        "assert torch.cuda.is_available(), \"no GPU found, in Colab go to 'Edit->Notebook settings' and choose a GPU hardware accelerator\"\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "class NeuralNgramDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"Iterates over ngrams in the input text data, returning the (n-1)gram\n",
        "    prefix as well as the target token.\"\"\"\n",
        "    def __init__(self, text_token_ids, n):\n",
        "        self.text_token_ids = text_token_ids\n",
        "        self.n = n\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text_token_ids)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        if i < self.n-1:\n",
        "            prev_token_ids = [vocab['<eos>']] * (self.n-i-1) + self.text_token_ids[:i]\n",
        "        else:\n",
        "            prev_token_ids = self.text_token_ids[i-self.n+1:i]\n",
        "\n",
        "        assert len(prev_token_ids) == self.n-1\n",
        "\n",
        "        x = torch.tensor(prev_token_ids)\n",
        "        y = torch.tensor(self.text_token_ids[i])\n",
        "        return x, y\n",
        "\n",
        "class NeuralNGramNetwork(nn.Module):\n",
        "    # a PyTorch Module that holds the neural network for your model\n",
        "\n",
        "    def __init__(self, n):\n",
        "        super().__init__()\n",
        "        self.n = n\n",
        "\n",
        "        self.fc1 = nn.Linear((n-1) * 128, 1024)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(1024, 1024)\n",
        "        self.fc3 = nn.Linear(1024, 128)\n",
        "        self.out = nn.Linear(128, len(word_types))\n",
        "        self.drop = nn.Dropout(p=0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x is a tensor of inputs with shape (batch, n-1)\n",
        "        # this function returns a tensor of log probabilities with shape (batch, vocab_size)\n",
        "\n",
        "        word_vec = F.embedding(x, self.out.weight).view(x.size()[0], -1)\n",
        "        # word_vec = self.embed(x) # (batch, n-1, 128)\n",
        "        concat_vec = torch.reshape(word_vec, (word_vec.size()[0], -1)) # (batch, (n-1) * 128)\n",
        "        h1 = self.drop(self.relu(self.fc1(concat_vec))) # (batch, 1024)\n",
        "        h2 = self.drop(self.relu(self.fc2(h1))) # (batch, 1024)\n",
        "        y = self.fc3(h2) # (batch, 128)\n",
        "        return self.out(y) # (batch, len(word_types))\n",
        "\n",
        "\n",
        "class NeuralNGramModel:\n",
        "    # a class that wraps NeuralNGramNetwork to handle training and evaluation\n",
        "    # it's ok if this doesn't work for unigram modeling\n",
        "    def __init__(self, n):\n",
        "        self.n = n\n",
        "        self.network = NeuralNGramNetwork(n).to(device)\n",
        "\n",
        "    def train(self):\n",
        "        dataset = NeuralNgramDataset(ids(train_text), self.n)\n",
        "        train_loader = torch.utils.data.DataLoader(dataset, batch_size=128, shuffle=True)\n",
        "        # iterating over train_loader with a for loop will return a 2-tuple of batched tensors\n",
        "        # the first tensor will be previous token ids with size (batch, n-1),\n",
        "        # and the second will be the current token id with size (batch, )\n",
        "        # you will need to move these tensors to GPU, e.g., by using the Tensor.cuda() function.\n",
        "\n",
        "        # this will take some time to run; use tqdm.notebook.tqdm to get a progress bar\n",
        "        # (see Assignment 0 for example)\n",
        "\n",
        "        # The basic recipe for training networks will be the same as assignment 0.\n",
        "        # A tutorial in Pytorch is also given here:\n",
        "        # https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#train-the-network\n",
        "\n",
        "        # You should also print the perplexity on the validation set after each epoch by\n",
        "        # calling self.perplexity(validation_text). You can do early stopping by\n",
        "        # comparing this perplexity to the perplexity from the previous epoch\n",
        "        # and stop training when it gets larger.\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "\n",
        "\n",
        "    def next_word_probabilities(self, text_prefix):\n",
        "        # you will need to convert text_prefix from strings to numbers with the `ids` function\n",
        "        # if your `perplexity` function below is based on a NeuralNgramDataset DataLoader, you will need to use the same strategy for prefixes with less than n-1 tokens to pass the validity check\n",
        "        #   the data loader appends extra \"<eos>\" (end of sentence) tokens to the start of the input so there are always enough to run the network\n",
        "        # if the text_prefix is longer than n-1 tokens, make sure you truncate it before passing to the network\n",
        "\n",
        "        # do a forward pass through the network and convert the log probabilities\n",
        "        # into probabilities before returning.\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "        # don't forget self.network.eval()\n",
        "        # don't forget to move tensors to the GPU\n",
        "\n",
        "    def perplexity(self, text):\n",
        "        # you may want to use a DataLoader here with a NeuralNgramDataset\n",
        "        # note that the NeuralNgramDataset prepends <eos> for the prefixes at the beginning less than n-1 tokens in length\n",
        "\n",
        "        valdataset = NeuralNgramDataset(ids(text), self.n)\n",
        "        val_loader = torch.utils.data.DataLoader(valdataset, batch_size=128, shuffle=False)\n",
        "\n",
        "        # Iterate over the val_loader, do a forward pass across the batches and\n",
        "        # compute the perplexities using the returned log probabilities.\n",
        "        # (Hint: you can use torch.nn.functional.nll_loss for computing perplexity;\n",
        "        # this will essentially select the log probability corresponding to the target word)\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "        # don't forget self.network.eval()\n",
        "        # don't forget to move tensors to the GPU\n",
        "\n",
        "neural_trigram_model = NeuralNGramModel(3)\n",
        "check_validity(neural_trigram_model)\n",
        "neural_trigram_model.train()\n",
        "print('neural trigram validation perplexity:', neural_trigram_model.perplexity(validation_text))\n",
        "\n",
        "save_truncated_distribution(neural_trigram_model, 'neural_trigram_predictions.npy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sm-xW4FGXYYi"
      },
      "source": [
        "Fill in your neural trigram perplexity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0cX0k2IW88k"
      },
      "source": [
        "<!-- Do not remove this comment, it is used by the autograder: RqYJKsoTS6 -->\n",
        "\n",
        "Neural trigram validation perplexity: ***fill in here***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8t5PCZnkB1r5"
      },
      "source": [
        "Free up RAM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1yH0lGOB1-S"
      },
      "outputs": [],
      "source": [
        "# Delete model we don't need.\n",
        "del neural_trigram_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRbS5nztWrEV"
      },
      "source": [
        "\n",
        "## Transformer Model\n",
        "\n",
        "For this stage of the project, you will implement Transformer language model.\n",
        "\n",
        "We will divide the text into sequences of length `max_len=128` and batch them together. We will use a similar batching strategy as done for recurrent language modeling: sentences are concatenated together so that one sentence starts right after the other, and an unfinished sentence will be continued in the next batch. The `TransformerLMDataset` implemented below does that for you.\n",
        "\n",
        "Recall that, for language modeling, we need to mask out the attention pattern such that every token only attends to preceding tokens and no other tokens which follow it. We will use the PyTorch `TransformerEncoder` module which implements this masking. You will need to provide a lower triangular mask of size `[max_len, max_len]` where all entries above the diagonal are -infinity and all entries on the diagonal or below are 0. Passing this to the `src_mask` argument of the `TransformerEncoder` will mask out the attention over future tokens (since the mask is *added* to the computed attention values).\n",
        "\n",
        "You will also need to implement positional embeddings -- for the purpose of this assignment you can implement lookup style learned embeddings (one for each position up till `max_len`). These should be added to the word embeddings before passing to the `TransformerEncoder`. (You can also try implementing the sinusoidal positional embeddings from the [Attention is all you need](https://arxiv.org/abs/1706.03762) paper if you wish)\n",
        "\n",
        "We expect your model to reach a validation perplexity below 135.  The following architecture and hyperparameters should be sufficient to get there.\n",
        "* 6 Transformer layers with 8 attention heads each and `d_model=256` and `dim_feedforward=512`\n",
        "* dropout of 0.3 within `TransformerEncoder`, after adding the positional embeddings, as well as at outputs of the `TransformerEncoder`\n",
        "* learned positional embeddings initialized uniformly in [-0.01, 0.01]. You can create these using `nn.Parameter`\n",
        "* instead of projecting directly from the Transformer output to the vocabulary size for softmax, project down to a smaller size first (e.g. 256->128->vocab_size).\n",
        "* use the same weights for the embedding layer and the pre-softmax layer; dimension 128. Similar to projecting the output of the transformer, you will need to project the embeddings at the input to the size of the model first 128->256. You can define a `nn.Linear` layer for the output and use `F.embedding` using its `.weight` to embed the inputs.\n",
        "* use a learning rate schedule which warms up the lr to a maximum value of 0.001 for the first 10% of steps, followed by linear decay back to 0 over the rest of the training. You can use `transformers.get_linear_schedule_with_warmup()` to implement this\n",
        "* train with Adam (default hyperparameters) for at least 20 epochs using a batch size of 64\n",
        "* use gradient clipping to limit the norm of the gradients to 1. You can use `torch.nn.utils.clip_grad_norm` just before you call `optimizer.step()`\n",
        "\n",
        "We encourage you to try other architectures and hyperparameters, and you will likely find some that work better than the ones listed above. A proper implementation with these should be enough to receive full credit on the assignment, though."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xcm-h0u-Wu73"
      },
      "outputs": [],
      "source": [
        "def ids(tokens):\n",
        "    return [vocab[t] for t in tokens]\n",
        "\n",
        "\n",
        "# YOU CAN DEFINE MORE HYPERPARAMETERS HERE\n",
        "batch_size = 64\n",
        "max_len = 128\n",
        "num_layers = 6\n",
        "hidden_size = 256\n",
        "num_heads = 8\n",
        "embed_size = 128\n",
        "ff_size = 512\n",
        "dropout = 0.3\n",
        "clip = 1.\n",
        "w_decay = 0.\n",
        "lr = 0.001\n",
        "n_epoch = 20\n",
        "\n",
        "assert torch.cuda.is_available(), \"no GPU found, in Colab go to 'Edit->Notebook settings' and choose a GPU hardware accelerator\"\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "class TransformerLMDataset:\n",
        "    \"\"\"Returns a batch of sequences with their corresponding target word ids.\n",
        "    Note that the returned tensors are of shapes `seq_len x batch_size`, hence\n",
        "    the sequence length dimension appears before the batch dimension.\"\"\"\n",
        "    def __init__(self, text_token_ids, bsz, max_len=128):\n",
        "        self.bsz = bsz\n",
        "        self.max_len = max_len\n",
        "        token_ids = torch.tensor(text_token_ids)\n",
        "        ncontig = token_ids.size(0) // bsz\n",
        "        token_ids = token_ids[:ncontig*bsz].view(bsz, -1) # bsz x ncontig\n",
        "        self.token_ids = token_ids.t().contiguous() # ncontig x bsz\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(math.ceil(self.token_ids.size(0) / self.max_len))\n",
        "\n",
        "    def __iter__(self):\n",
        "        for i in range(0, self.token_ids.size(0)-1, self.max_len):\n",
        "            seqlen = min(self.max_len, self.token_ids.size(0) - i - 1)\n",
        "            x = self.token_ids[i:i+seqlen] # max_len x bsz\n",
        "            y = self.token_ids[i+1:i+seqlen+1] # max_len x bsz\n",
        "            yield x, y\n",
        "\n",
        "class TransformerNetwork(nn.Module):\n",
        "    # a PyTorch Module that holds the neural network for your model\n",
        "\n",
        "    def __init__(self, max_len=128, embed_size=128):  # feel free to add more parameters\n",
        "        super().__init__()\n",
        "\n",
        "        # Initialize the different layers needed in the computation graph.\n",
        "        # A full list of available layers in Pytorch is given here:\n",
        "        # https://pytorch.org/docs/stable/nn.html\n",
        "\n",
        "        # In PyTorch, to construct a TransformerEncoder layer, we first need\n",
        "        # to construct a nn.TransformerEncoderLayer and then pass it to\n",
        "        # nn.TransformerEncoder.\n",
        "        # Besides these you will need linear and dropout layers for this model.\n",
        "\n",
        "        # Below we have initialized the positional embeddings for you.\n",
        "        self.pos_embed = nn.Parameter(torch.FloatTensor(max_len, embed_size).uniform_(-0.01, 0.01))\n",
        "\n",
        "        self.in_proj = nn.Linear(embed_size, hidden_size)\n",
        "        self.pos_drop = nn.Dropout(p=dropout)\n",
        "        self.enc_drop = nn.Dropout(p=dropout)\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=hidden_size,\n",
        "            nhead=num_heads,\n",
        "            dim_feedforward=ff_size,\n",
        "            dropout=dropout,\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "        self.out_proj = nn.Linear(hidden_size, embed_size)\n",
        "        self.out = nn.Linear(embed_size, len(word_types), bias=False)\n",
        "        \n",
        "\n",
        "\n",
        "    def _generate_mask(self, sz):\n",
        "        \"\"\"Return an attention mask with shape (sz, sz).\"\"\"\n",
        "\n",
        "        # Note: you can use torch.triu (https://pytorch.org/docs/stable/generated/torch.triu.html) to build a upper triangular matrix.\n",
        "        # For passing to the TransformerEncoder, we will need to replace 0s with -inf and 1s with 0\n",
        "\n",
        "        mask = torch.triu(torch.ones(sz, sz), diagonal=1)\n",
        "        mask = mask.masked_fill(mask == 1, float('-inf'))\n",
        "        return mask\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\" x - a tensor with shape (seq_len, bsz)\n",
        "            returns a tensor of log probabilities with shape (seq_len, bsz, len(word_types)).\n",
        "        \"\"\"\n",
        "\n",
        "        # Make sure you add the positional embeddings initialized above to the word embeddings\n",
        "        # extracted from the inputs.\n",
        "        # Make sure you provide an attention mask to the TransformerEncoder (after moving it to the GPU).\n",
        "        seq_len = x.size(0)\n",
        "        word_vec = F.embedding(x, self.out.weight)\n",
        "        pos = self.pos_embed[:seq_len].unsqueeze(1)\n",
        "        word_vec = self.pos_drop(word_vec + pos)\n",
        "        hidden = self.in_proj(word_vec)\n",
        "\n",
        "        mask = self._generate_mask(seq_len).to(x.device)\n",
        "        enc_out = self.transformer(hidden, mask)\n",
        "        enc_out = self.enc_drop(enc_out)\n",
        "\n",
        "        proj = self.out_proj(enc_out)\n",
        "        logits = self.out(proj)\n",
        "        return F.log_softmax(logits, dim=-1)\n",
        "\n",
        "\n",
        "class TransformerModel:\n",
        "    \"A class that wraps TransformerNetwork to handle training and evaluation.\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.network = TransformerNetwork().cuda()\n",
        "\n",
        "    def train(self):\n",
        "        transformer_dataset = TransformerLMDataset(ids(train_text), batch_size, max_len)\n",
        "        # Iterate over transformer_dataset with a for loop (optionally with tqdm).\n",
        "        # Looping thru this dataset gives (x, y) tuples,\n",
        "        # where x is a seqlen x batch_size token id tensor, and y is a seqlen x batch_size token id tensor.\n",
        "        # The token ids in y are the next word targets for the sequence up till that position\n",
        "        # in x.\n",
        "\n",
        "        # The basic recipe for training the network will be the same as the NeuralNGramModel,\n",
        "        # but note that the network will return the next word predictions of the entire\n",
        "        # sequence together. You will need to reshape this in order to use the nll_loss.\n",
        "\n",
        "        # Note: You will need a learning rate scheduler in addition to the optimizer.\n",
        "        # Note: Initialize your optimizer and scheduler before starting the training loop.\n",
        "        # Note: Make sure you update the parameters of your optimizer and scheduler after each training step.\n",
        "        # Note: You should zero out previous gradients before each backpropragation:\n",
        "        # https://pytorch.org/docs/stable/generated/torch.optim.Optimizer.zero_grad.html\n",
        "\n",
        "        # You should also print the perplexity on the validation set after each epoch by\n",
        "        # calling self.perplexity(validation_text). You can do early stopping by\n",
        "        # comparing this perplexity to the perplexity from the previous epoch\n",
        "        # and stop training when it gets larger.\n",
        "\n",
        "        # You should also clip the gradients before performing an optimizer.step()\n",
        "        # using torch.nn.utils.clip_grad_norm.\n",
        "\n",
        "        optimizer = optim.Adam(self.network.parameters(), lr=lr, weight_decay=w_decay)\n",
        "        total_steps = len(transformer_dataset) * n_epoch\n",
        "        warmup_steps = max(1, int(0.1 * total_steps))\n",
        "        scheduler = transformers.get_linear_schedule_with_warmup(\n",
        "            optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps\n",
        "        )\n",
        "\n",
        "        prev_ppl = None\n",
        "        for epoch in range(n_epoch):\n",
        "            self.network.train()\n",
        "            for x, y in tqdm.notebook.tqdm(transformer_dataset, leave=False):\n",
        "                x = x.cuda()\n",
        "                y = y.cuda()\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                log_probs = self.network(x)\n",
        "                loss = F.nll_loss(log_probs.view(-1, log_probs.size(-1)), y.reshape(-1))\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm(self.network.parameters(), clip)\n",
        "                optimizer.step()\n",
        "                scheduler.step()\n",
        "\n",
        "            val_ppl = self.perplexity(validation_text)\n",
        "            print(f'epoch {epoch+1} validation perplexity: {val_ppl}')\n",
        "            if prev_ppl is not None and val_ppl > prev_ppl:\n",
        "                break\n",
        "            prev_ppl = val_ppl\n",
        "\n",
        "\n",
        "    def next_word_probabilities(self, text_prefix):\n",
        "        \"Return a list of probabilities for each word in the vocabulary.\"\n",
        "\n",
        "        # you will need to convert text_prefix from strings to numbers with the `ids` function\n",
        "        # We won't be calling check_validity on the Transformer so you don't need\n",
        "        # worry about the empty text_prefix.\n",
        "\n",
        "        # Make sure you return probabilities instead of log probabilities!\n",
        "\n",
        "        if len(text_prefix) == 0:\n",
        "            prefix_ids = [vocab['<eos>']]\n",
        "        else:\n",
        "            prefix_ids = ids(text_prefix[-max_len:])\n",
        "\n",
        "        x = torch.tensor(prefix_ids, dtype=torch.long).unsqueeze(1).cuda()\n",
        "        self.network.eval()\n",
        "        with torch.no_grad():\n",
        "            log_probs = self.network(x)\n",
        "            probs = torch.exp(log_probs[-1, 0]).detach().cpu().numpy()\n",
        "        return probs.tolist()\n",
        "\n",
        "\n",
        "    def perplexity(self, text):\n",
        "        \"Return perplexity as a float.\"\n",
        "        # Use torch.no_grad() for extra speed.\n",
        "\n",
        "        # Note that the nll_loss function, by default, computes the losses\n",
        "        # averaged over each loss element in the batch.\n",
        "\n",
        "        bsz = batch_size if len(text) > batch_size else 1\n",
        "        transformer_dataset = TransformerLMDataset(ids(text), bsz, min(max_len, len(text)))\n",
        "\n",
        "        # Loop over the transformer_dataset and get the outputs from your network.\n",
        "        # Use nll_loss after reshaping these outputs and the corresponding targets\n",
        "        # to get the average log probabilities.\n",
        "\n",
        "        # Note that nll_loss by default will give you an average over all the elements\n",
        "        # in a batch, keep track of the total number of elements for computing the\n",
        "        # overall average for the perplexity.\n",
        "\n",
        "        self.network.eval()\n",
        "        total_loss = 0.0\n",
        "        total_count = 0\n",
        "        with torch.no_grad():\n",
        "            for x, y in transformer_dataset:\n",
        "                x = x.cuda()\n",
        "                y = y.cuda()\n",
        "                log_probs = self.network(x)\n",
        "                loss = F.nll_loss(\n",
        "                    log_probs.view(-1, log_probs.size(-1)),\n",
        "                    y.reshape(-1),\n",
        "                    reduction='sum'\n",
        "                )\n",
        "                total_loss += loss.item()\n",
        "                total_count += y.numel()\n",
        "\n",
        "        return math.exp(total_loss / total_count)\n",
        "\n",
        "\n",
        "transformer_model = TransformerModel()\n",
        "transformer_model.train()\n",
        "\n",
        "print('transformer validation perplexity:', transformer_model.perplexity(validation_text))\n",
        "save_truncated_distribution(transformer_model, 'transformer_predictions.npy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLoiXBWMaSPc"
      },
      "source": [
        "# Experimentation: 2-Page Report\n",
        "\n",
        "Now it's time for you to experiment.  Try to reach a validation perplexity below 120. You may either modify the transformer class above, or copy it to a new code cell and modify it there. Just **be sure to run the new code cell to generate results with your improved transformer**.  \n",
        "\n",
        "It is okay if the bulk of your improvements are due to hyperparameter tuning (such as tuning the number or sizes of layers and attention heads, dropout, batch size, or the learning rate schedule). You are encouraged to explore the ideas below to further boost your performance. Here are some ideas:\n",
        "* UID regularization - regularization based on the uniform information density (UID) hypothesis has shown improvement in language modeling. Check out https://arxiv.org/pdf/2105.07144.pdf for details.\n",
        "* other types of regularization from https://arxiv.org/pdf/1708.02182.pdf, such as activation regularization, weight-drop regularization, embedding dropout. Note that these have been tested for LSTMs, so they may or may not work for transformers, but you can try them out.\n",
        "* adaptive input representations (https://arxiv.org/pdf/1809.10853.pdf) which assign larger embeddings to frequent words\n",
        "* ensembling - average the predictions of several models trained with different initialization random seeds\n",
        "\n",
        "You may notice that most of these suggestions are regularization techniques.  This dataset is considered fairly small, so regularization is one of the best ways to improve performance.\n",
        "\n",
        "For this section, you will submit a write-up describing the extensions and/or modifications that you tried.  Your write-up should be **2-page maximum** in length and should be submitted in PDF format. You may use any editor you like, but we recommend using LaTeX and working in an environment like Overleaf.\n",
        "\n",
        "On the **first** page, please include the following for full credit (25 pts):\n",
        "1.   A concise and precise description of the extension that you tried.\n",
        "2.   A motivation for why you believed this approach might improve your model.\n",
        "3.   A discussion of whether the extension was effective and/or an analysis of the results.  This will generally involve some combination of tables, learning curves, etc.\n",
        "4.   A bottom-line summary of your results comparing validation perplexities of your improvement to the original transformer.\n",
        "\n",
        "On the **second** page, please include the following for full credit (5 pts):\n",
        "1. A discussion of errors and bugs you have encountered.\n",
        "2. Your learning experiences from this implementation.\n",
        "\n",
        "The **first page (25%)** will be assessed on the clarity, correctness, and interestingness of your experiment and analysis.\n",
        "\n",
        "The **second page (5%)** will be graded based on completion: **a reasonable discussion of your errors and learning experience will be enough to collect full credit.**\n",
        "\n",
        "The purpose of this exercise is to experiment, so feel free to try/ablate multiple of the suggestions above as well as any others you come up with! Even if your idea does not work, we would like to see a clear and concise description of what you tried and what were the results.\n",
        "When you submit the file, please name it `report.pdf`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHTOfrCG8CRF"
      },
      "source": [
        "### Submission\n",
        "\n",
        "Upload a submission with the following files to Gradescope:\n",
        "* assignment_1.ipynb (rename to match this exactly)\n",
        "* transformer_predictions.npy (this should also include all improvements from your exploration)\n",
        "* neural_trigram_predictions.npy\n",
        "* trigram_backoff_predictions.npy\n",
        "* bigram_predictions.npy\n",
        "* report.pdf\n",
        "\n",
        "You can upload files individually or as part of a zip file, but if using a zip file be sure you are zipping the files directly and not a folder that contains them.\n",
        "\n",
        "Be sure to check the output of the autograder after it runs.  It should confirm that no files are missing and that the output files have the correct format.  Note that the test set perplexities shown by the autograder are on a different scale from your validation set perplexities due to selecting different text and truncating the distribution.  Don't worry if the values seem worse. We will compare your perplexity on the test set to our model's perplexity and assign a score based on that."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
